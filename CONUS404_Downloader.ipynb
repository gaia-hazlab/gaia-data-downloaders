{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e6b60-4cd3-4341-942b-f45b4209e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "! /home/balaji24/.pixi/bin/pixi add s3fs regionmask geopandas rioxarray pyproj intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e7cd42-c69e-42d6-ba95-3f895fd9d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "! /home/balaji24/.pixi/bin/pixi add s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39acaef-77d7-40d6-9c4e-a66f70fd64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROJ_LIB\"] = \"/home/balaji24/skagit-met/.pixi/envs/analysis/share/proj\"\n",
    "os.environ[\"GDAL_DATA\"] = \"/home/balaji24/skagit-met/.pixi/envs/analysis/share/gdal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ad6a7-c859-4638-9161-bd3997d5404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import regionmask\n",
    "from shapely.geometry import shape, Polygon, LinearRing\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1dc013-1d0d-4380-bf45-6f1687f6300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "DATASET_KIND = \"daily\"   # or \"hourly\"\n",
    "START = \"2014-10-01\"\n",
    "END   = \"2023-12-31\"\n",
    "\n",
    "VARS = [\n",
    "    \"T2\", \"Q2\", \"U10\", \"V10\", \"PSFC\",\n",
    "    \"SWDNB\", \"LWDNB\",\n",
    "    \"RAINC\", \"RAINNC\", \"SNOWNC\",\n",
    "    \"SMOIS\", \"TSLB\", \"HFX\", \"LH\", \"HGT\",\n",
    "]\n",
    "\n",
    "BOUNDARY_JSON = \"../data/GIS/SkagitBoundary.json\" \n",
    "\n",
    "OUT_ZARR = Path(\"../../../../data0/balaji24/data/CONUS/conus404_skagit_daily.zarr\") if DATASET_KIND==\"daily\" \\\n",
    "           else Path(\"../../../../data0/balaji24/data/CONUS/conus404_skagit_hourly.zarr\")\n",
    "\n",
    "# Check file existence\n",
    "assert Path(BOUNDARY_JSON).exists(), f\"Boundary file not found: {BOUNDARY_JSON}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9f0eb-cb52-4d2d-b6d8-f89fb5bc778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open CONUS404 from OSN\n",
    "\n",
    "endpoint = \"https://usgs.osn.mghpcc.org\"\n",
    "store_url = {\n",
    "    \"daily\":  \"s3://hytest/conus404/conus404_daily.zarr\",\n",
    "    \"hourly\": \"s3://hytest/conus404/conus404_hourly.zarr\",\n",
    "}[DATASET_KIND]\n",
    "\n",
    "ds = xr.open_zarr(\n",
    "    store=store_url,\n",
    "    storage_options={\"anon\": True, \"client_kwargs\": {\"endpoint_url\": endpoint}},\n",
    "    consolidated=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec8b74-8370-4928-a79d-70f082fbb1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset time & select variables\n",
    "\n",
    "ds = ds.sel(time=slice(START, END))\n",
    "available = set(ds.data_vars)\n",
    "keep = [v for v in VARS if v in available]\n",
    "if not keep:\n",
    "    raise ValueError(f\"None of {VARS} found in dataset. Available: {list(ds.data_vars)[:25]}\")\n",
    "ds = ds[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb13cf5-18a5-4636-8486-789a712bf477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify lat/lon grid\n",
    "\n",
    "if \"lat\" in ds and \"lon\" in ds:\n",
    "    da_lat, da_lon = ds[\"lat\"], ds[\"lon\"]\n",
    "else:\n",
    "    candidates = [(\"XLAT\",\"XLONG\"), (\"latitude\",\"longitude\")]\n",
    "    for a,b in candidates:\n",
    "        if a in ds and b in ds:\n",
    "            da_lat, da_lon = ds[a], ds[b]\n",
    "            break\n",
    "    else:\n",
    "        raise KeyError(\"Could not find lat/lon variables in dataset.\")\n",
    "\n",
    "spatial_dims = da_lat.dims  # ('y','x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe8aa7-5f7c-4a98-938e-e94617c72baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Skagit boundary JSON\n",
    "\n",
    "def load_boundary_to_gdf(json_path: str) -> gpd.GeoDataFrame:\n",
    "    with open(json_path) as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    basin_geom = None\n",
    "    if isinstance(obj, dict) and \"type\" in obj:\n",
    "        try:\n",
    "            if obj[\"type\"] == \"FeatureCollection\":\n",
    "                geom = shape(obj[\"features\"][0][\"geometry\"])\n",
    "            elif obj[\"type\"] == \"Feature\":\n",
    "                geom = shape(obj[\"geometry\"])\n",
    "            else:\n",
    "                geom = shape(obj)\n",
    "            basin_geom = geom\n",
    "        except Exception:\n",
    "            basin_geom = None\n",
    "\n",
    "    if basin_geom is None and isinstance(obj, dict) and \"lon\" in obj and \"lat\" in obj:\n",
    "        coords = list(zip(obj[\"lon\"], obj[\"lat\"]))\n",
    "        if coords[0] != coords[-1]:\n",
    "            coords.append(coords[0])\n",
    "        basin_geom = Polygon(LinearRing(coords))\n",
    "\n",
    "    if basin_geom is None:\n",
    "        raise ValueError(\"Invalid boundary file format. Must be GeoJSON or {'lon','lat'} arrays.\")\n",
    "\n",
    "    return gpd.GeoDataFrame({\"name\":[\"SkagitBoundary\"]}, geometry=[basin_geom], crs=\"EPSG:4326\")\n",
    "\n",
    "gdf = load_boundary_to_gdf(BOUNDARY_JSON).explode(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30652c3-8bd6-48a2-a1d0-8f0fe5c8b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply spatial mask for Skagit\n",
    "\n",
    "if hasattr(regionmask.Regions, \"from_geopandas\"):\n",
    "    regions = regionmask.Regions.from_geopandas(gdf, names=\"name\")\n",
    "else:\n",
    "    outlines = [geom for geom in gdf.geometry if geom is not None]\n",
    "    regions  = regionmask.Regions(outlines=outlines, names=[\"SkagitBoundary\"]*len(outlines),\n",
    "                                  numbers=list(range(len(outlines))), name=\"SkagitBoundary\")\n",
    "\n",
    "mask = regions.mask(ds[\"lon\"], ds[\"lat\"])\n",
    "ds = ds.where(mask.notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d0a19-35f9-4a7d-be6b-668f74db5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived diagnostics\n",
    "\n",
    "if {\"U10\",\"V10\"} <= set(ds.data_vars):\n",
    "    ds[\"WS10\"] = (ds[\"U10\"]**2 + ds[\"V10\"]**2)**0.5\n",
    "    ds[\"WS10\"].attrs.update(units=\"m s-1\", long_name=\"10 m wind speed\")\n",
    "\n",
    "has_rain = {\"RAINC\",\"RAINNC\"} <= set(ds.data_vars)\n",
    "has_snow = \"SNOWNC\" in ds.data_vars\n",
    "if (has_rain or has_snow):\n",
    "    pieces = []\n",
    "    if has_rain: pieces.append(ds[\"RAINC\"] + ds[\"RAINNC\"])\n",
    "    if has_snow: pieces.append(ds[\"SNOWNC\"])\n",
    "    ds[\"PRECIP_TOT\"] = sum(pieces)\n",
    "    ds[\"PRECIP_TOT\"].attrs.update(units=\"mm\", long_name=\"Total precipitation (rain+snow)\")\n",
    "\n",
    "if {\"T2\",\"Q2\",\"PSFC\"} <= set(ds.data_vars):\n",
    "    T_c = ds[\"T2\"] - 273.15\n",
    "    p_kpa = ds[\"PSFC\"]/1000.0\n",
    "    q = ds[\"Q2\"]\n",
    "    es_kpa = 0.6108 * np.exp((17.27*T_c)/(T_c+237.3))\n",
    "    e_kpa = (q*p_kpa)/(0.622 + 0.378*q)\n",
    "    ds[\"RH\"] = (e_kpa/es_kpa).clip(0,1)*100.0\n",
    "    ds[\"VPD\"] = (es_kpa - e_kpa).clip(min=0)\n",
    "    ds[\"RH\"].attrs.update(units=\"%\", long_name=\"Relative Humidity\")\n",
    "    ds[\"VPD\"].attrs.update(units=\"kPa\", long_name=\"Vapor Pressure Deficit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339598e-033f-485f-b9d3-f35a51720ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking + encoding cleanup\n",
    "\n",
    "CHUNKS = {\"time\": 30, spatial_dims[0]: 300, spatial_dims[1]: 300}\n",
    "if DATASET_KIND == \"hourly\":\n",
    "    CHUNKS[\"time\"] = 24*7\n",
    "ds = ds.chunk(CHUNKS)\n",
    "\n",
    "# remove bad encodings (prevents overlapping chunk error)\n",
    "for v in ds.variables:\n",
    "    ds[v].encoding.pop(\"chunks\", None)\n",
    "\n",
    "if \"lat\" in ds:\n",
    "    ds[\"lat\"] = ds[\"lat\"].chunk({spatial_dims[0]:300, spatial_dims[1]:300})\n",
    "if \"lon\" in ds:\n",
    "    ds[\"lon\"] = ds[\"lon\"].chunk({spatial_dims[0]:300, spatial_dims[1]:300})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a98141f-793e-4d8a-92a0-4ad65000b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Zarr (v2, with progress bar)\n",
    "\n",
    "OUT_ZARR.parent.mkdir(parents=True, exist_ok=True)\n",
    "with ProgressBar():\n",
    "    ds.to_zarr(OUT_ZARR, mode=\"w\", consolidated=True, zarr_version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d1c18-3bf3-49f2-8fa7-c101afdb889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC Visualization\n",
    "\n",
    "saved = xr.open_zarr(OUT_ZARR)\n",
    "print(\"Saved variables:\", list(saved.data_vars))\n",
    "print(\"Dataset sizes:\", saved.sizes)\n",
    "\n",
    "plot_var = next((v for v in [\"T2\",\"PRECIP_TOT\",\"SWDNB\",\"WS10\"] if v in saved.data_vars), list(saved.data_vars)[0])\n",
    "ts_var = \"T2\" if \"T2\" in saved.data_vars else plot_var\n",
    "ts = saved[ts_var].mean(dim=[spatial_dims[0], spatial_dims[1]])\n",
    "\n",
    "fig = plt.figure(figsize=(12,4.5))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "saved[plot_var].isel(time=0).plot(ax=ax1)\n",
    "gdf.boundary.plot(ax=ax1, color=\"k\")\n",
    "ax1.set_title(f\"{plot_var} (first timestep)\")\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ts.plot(ax=ax2)\n",
    "ax2.set_title(f\"Basin-mean {ts_var} over time\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c26b67-94d0-4d25-a60a-7a1af383ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(ds.data_vars))[:50])\n",
    "missing = [v for v in [\"RAINC\",\"RAINNC\",\"SNOWNC\"] if v not in ds]\n",
    "print(\"Missing precip vars:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50900366-9198-43e4-b892-5d1a44adf101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs, itertools, xarray as xr\n",
    "\n",
    "endpoint = \"https://usgs.osn.mghpcc.org\"\n",
    "fs = s3fs.S3FileSystem(anon=True, client_kwargs={\"endpoint_url\": endpoint})\n",
    "\n",
    "# 1) Explore likely roots\n",
    "roots = [\n",
    "    \"hytest/conus404\",\n",
    "    \"hytest/CONUS404\",\n",
    "    \"hytest/CONUS_404\",\n",
    "    \"hytest/conus404/wrfout\",\n",
    "    \"hytest/CONUS404/wrfout\",\n",
    "    \"hytest/CONUS_404/wrfout\",\n",
    "]\n",
    "existing = []\n",
    "for r in roots:\n",
    "    try:\n",
    "        fs.ls(r)\n",
    "        existing.append(r)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Existing roots:\", existing or \"(none)\")\n",
    "\n",
    "# 2) Try to *find* any wrfout NetCDFs by scanning a few depths\n",
    "candidates = []\n",
    "for r in existing or [\"hytest\"]:\n",
    "    try:\n",
    "        # limit depth to keep it fast; increase maxdepth if needed\n",
    "        for path in fs.find(r, maxdepth=6):\n",
    "            if path.lower().endswith(\".nc\") and \"wrfout\" in path.lower():\n",
    "                candidates.append(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"Found {len(candidates)} wrfout .nc files (showing 10):\")\n",
    "for p in candidates[:10]:\n",
    "    print(\" -\", p)\n",
    "\n",
    "# 3) If none yet, try a broader search but with a safety cap\n",
    "if not candidates:\n",
    "    hits = []\n",
    "    for path in fs.find(\"hytest\", maxdepth=7):\n",
    "        if path.lower().endswith(\".nc\") and \"wrfout\" in path.lower():\n",
    "            hits.append(path)\n",
    "            if len(hits) >= 20:\n",
    "                break\n",
    "    candidates = hits\n",
    "    print(f\"\\nBroader scan found {len(candidates)} candidates (showing 10):\")\n",
    "    for p in candidates[:10]:\n",
    "        print(\" -\", p)\n",
    "\n",
    "# 4) Open one file and list precip-like variables\n",
    "if not candidates:\n",
    "    raise SystemExit(\"No wrfout NetCDFs found under hytest (at searched depths). Try increasing maxdepth or different roots.\")\n",
    "\n",
    "sample = candidates[0]\n",
    "print(\"\\nOpening sample:\", sample)\n",
    "\n",
    "url = f\"simplecache::s3://{sample}\"\n",
    "ds = xr.open_dataset(\n",
    "    url,\n",
    "    engine=\"h5netcdf\",\n",
    "    backend_kwargs={\"phony_dims\": \"access\"},\n",
    "    storage_options={\"anon\": True, \"client_kwargs\": {\"endpoint_url\": endpoint}},\n",
    ")\n",
    "\n",
    "prec_vars = [v for v in ds.data_vars if any(k in v.upper() for k in [\"RAIN\",\"SNOW\",\"APCP\",\"PREC\"])]\n",
    "print(\"Precip-like vars in sample:\", prec_vars)\n",
    "\n",
    "# If you see RAINNC/RAINC/SNOWNC, youâ€™re good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ac00a-7edb-4f82-9df5-47759f4d7195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixi: skagit-analysis",
   "language": "python",
   "name": "skagit-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
