{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f615f93",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f51ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from obspy.clients.fdsn import Client\n",
    "from shapely.geometry import Point, box\n",
    "\n",
    "from pygeohydro import WBD\n",
    "\n",
    "#warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "#warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d0bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_huc_geometries_local(huc_level, geom_gdf):\n",
    "    \"\"\"\n",
    "    Fetch HUC geometries locally using USGS WBD via pygeohydro.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    huc_level : str or int\n",
    "        HUC level ('02','04','06','08','10','12' or 2,4,6,8,10,12)\n",
    "    geom_gdf : geopandas.GeoDataFrame\n",
    "        GeoDataFrame containing geometry (e.g., bbox or AOI)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    geopandas.GeoDataFrame\n",
    "        HUC geometries intersecting the input geometry\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize HUC level\n",
    "    huc_level = f\"huc{int(huc_level)}\"\n",
    "\n",
    "    # Initialize WBD\n",
    "    wbd = WBD(huc_level)\n",
    "\n",
    "    # Extract shapely geometry (first feature)\n",
    "    geom = geom_gdf.geometry.iloc[0]\n",
    "\n",
    "    # Fetch HUCs intersecting geometry\n",
    "    huc_gdf = wbd.bygeom(geom)\n",
    "\n",
    "    return huc_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54059af",
   "metadata": {},
   "source": [
    "## User Configuration\n",
    "\n",
    "Basins are auto-discovered from basin names by querying USGS WBD HUC geometries using `easysnowdata`.\n",
    "\n",
    "Notes:\n",
    "- This approach uses Google Earth Engine under the hood; you may need to authenticate once.\n",
    "- `bbox_wa` should tightly bound your study region to keep queries fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f04afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  HUC Level: 10\n",
      "  FDSN Provider: IRIS\n",
      "  Bounding Box: (-124.8, 45.5, -120.0, 49.0)\n",
      "  Elevation Filter: None to None\n",
      "  Network Filter: ['UW', 'CC']\n",
      "  Output Directory: /Users/marinedenolle/GitHub/gaia-data-downloaders\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "\n",
    "# List of basin names to search for\n",
    "basin_names = [\n",
    "    \"Skagit\",\n",
    "    \"Nooksack\",\n",
    "    \"Skykomish\",\n",
    "    \"Snoqualmie\",\n",
    "    \"Cedar\",  # Cedar River watershed (not Cedar Creek)\n",
    "    \"Green\",\n",
    "    \"Puyallup\",\n",
    "    \"Carbon\",\n",
    "    \"Nisqually\",\n",
    "    \"Cowlitz\",\n",
    "]\n",
    "\n",
    "# HUC level to query (8, 10, or 12)\n",
    "huc_level = 10\n",
    "\n",
    "# FDSN client for station queries\n",
    "fdsn_provider = \"IRIS\"\n",
    "\n",
    "# Bounding box for western Washington (minlon, minlat, maxlon, maxlat)\n",
    "# This should encompass all basins of interest\n",
    "bbox_wa = (-124.8, 45.5, -120.0, 49.0)\n",
    "\n",
    "# Optional: Filter stations by elevation (meters)\n",
    "# Set to None to disable filtering\n",
    "elev_min = None  # Example: 0 (sea level)\n",
    "elev_max = None  # Example: 3000 (3000 meters)\n",
    "\n",
    "# Optional: Filter by specific networks\n",
    "# Set to None to include all networks\n",
    "network_filter = [\"UW\", \"CC\"]  # UW + CC (as requested)\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\".\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  HUC Level: {huc_level}\")\n",
    "print(f\"  FDSN Provider: {fdsn_provider}\")\n",
    "print(f\"  Bounding Box: {bbox_wa}\")\n",
    "print(f\"  Elevation Filter: {elev_min} to {elev_max}\")\n",
    "print(f\"  Network Filter: {network_filter}\")\n",
    "print(f\"  Output Directory: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72aa2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUC level to query (8, 10, or 12)\n",
    "huc_level = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b7c1fc",
   "metadata": {},
   "source": [
    "## Step 1: Discover HUC Codes from Basin Names\n",
    "\n",
    "Query the USGS WBD database to find HUC codes for each basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "305a6cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Discovering HUC codes for basins...\n",
      "Searching for 10 basins in western Washington\n",
      "\n",
      "Using HUC level: 10 (code column: huc10)\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Discovering HUC codes for basins...\")\n",
    "print(f\"Searching for {len(basin_names)} basins in western Washington\\n\")\n",
    "\n",
    "# HUC level string expected by easysnowdata (e.g., '08', '10', '12')\n",
    "huc_level_str = f\"{huc_level:02d}\"\n",
    "code_col = f\"huc{int(huc_level_str)}\"\n",
    "\n",
    "# Create geometry from bounding box (box already imported from shapely.geometry)\n",
    "bbox_geom = box(*bbox_wa)\n",
    "\n",
    "print(f\"Using HUC level: {huc_level_str} (code column: {code_col})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea9846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49c89dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinedenolle/GitHub/gaia-data-downloaders/.pixi/envs/default/lib/python3.13/site-packages/pygeoogc/core.py:293: UserWarning: Found 1 failed request. Retrying ...\n",
      "  resp = self._cleanup_resp(resp, payloads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 288 entries, 0 to 287\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   geometry           288 non-null    geometry\n",
      " 1   objectid           288 non-null    int64   \n",
      " 2   tnmid              288 non-null    object  \n",
      " 3   metasourceid       287 non-null    object  \n",
      " 4   sourcedatadesc     284 non-null    object  \n",
      " 5   sourceoriginator   284 non-null    object  \n",
      " 6   sourcefeatureid    0 non-null      object  \n",
      " 7   loaddate           288 non-null    int64   \n",
      " 8   referencegnis_ids  280 non-null    object  \n",
      " 9   areaacres          288 non-null    float64 \n",
      " 10  areasqkm           288 non-null    float64 \n",
      " 11  states             288 non-null    object  \n",
      " 12  huc10              288 non-null    object  \n",
      " 13  name               288 non-null    object  \n",
      " 14  hutype             288 non-null    object  \n",
      " 15  humod              43 non-null     object  \n",
      " 16  globalid           288 non-null    object  \n",
      " 17  shape_Length       288 non-null    float64 \n",
      " 18  shape_Area         288 non-null    float64 \n",
      "dtypes: float64(4), geometry(1), int64(2), object(12)\n",
      "memory usage: 42.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Warning Slow (5min+) for HUC10 over large area.\n",
    "hucs_all = get_huc_geometries_local(\n",
    "    huc_level=huc_level_str,\n",
    "    geom_gdf=gpd.GeoDataFrame(geometry=[bbox_geom], crs=\"EPSG:4326\"),\n",
    ")\n",
    "hucs_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e86b8db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hucs_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Filter HUCs to only those matching our prescribed basin names\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m name_series = \u001b[43mhucs_all\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m      3\u001b[39m mask = pd.Series([\u001b[38;5;28;01mFalse\u001b[39;00m] * \u001b[38;5;28mlen\u001b[39m(hucs_all), index=hucs_all.index)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m basin_name \u001b[38;5;129;01min\u001b[39;00m basin_names:\n",
      "\u001b[31mNameError\u001b[39m: name 'hucs_all' is not defined"
     ]
    }
   ],
   "source": [
    "# Filter HUCs to only those matching our prescribed basin names\n",
    "name_series = hucs_all['name'].astype(str)\n",
    "mask = pd.Series([False] * len(hucs_all), index=hucs_all.index)\n",
    "\n",
    "for basin_name in basin_names:\n",
    "    mask |= name_series.str.contains(basin_name, case=False, na=False)\n",
    "\n",
    "hucs_filtered = hucs_all[mask].copy()\n",
    "\n",
    "print(f\"Filtered from {len(hucs_all)} to {len(hucs_filtered)} HUC polygons matching basin names\")\n",
    "print(f\"Basin names searched: {basin_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed4080a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geometry             POLYGON ((-119.89030507051542 47.7960916326412...\n",
       "objectid                                                           746\n",
       "tnmid                           {D1CF8F04-E197-4E10-8B0D-DDF136689619}\n",
       "metasourceid                    {511D2AC8-11BA-45FC-AB98-F69D693D4C44}\n",
       "sourcedatadesc                        Watershed Boundary Dataset (WBD)\n",
       "sourceoriginator     Natural Resources and Conservation Service and...\n",
       "sourcefeatureid                                                   None\n",
       "loaddate                                                 1723726881000\n",
       "referencegnis_ids                                              1518895\n",
       "areaacres                                                     131137.1\n",
       "areasqkm                                                        530.69\n",
       "states                                                              WA\n",
       "huc10                                                       1702001203\n",
       "name                                               Upper Douglas Creek\n",
       "hutype                                                               S\n",
       "humod                                                             None\n",
       "globalid                        {1C14CF67-E29C-11E2-8094-0021280458E6}\n",
       "shape_Length                                             190420.833444\n",
       "shape_Area                                            1167129270.76913\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at one to understand structure\n",
    "hucs_all.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0570781e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hucs_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mhucs_all\u001b[49m.explore()\n",
      "\u001b[31mNameError\u001b[39m: name 'hucs_all' is not defined"
     ]
    }
   ],
   "source": [
    "hucs_all.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7569da8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hucs_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mhucs_all\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m HUC polygon(s) in bbox.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mColumns:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlist\u001b[39m(hucs_all.columns))\n",
      "\u001b[31mNameError\u001b[39m: name 'hucs_all' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(hucs_all)} HUC polygon(s) in bbox.\")\n",
    "print('Columns:', list(hucs_all.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c058943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching basin names to HUC polygons...\n",
      "  Skagit         : ‚ö†Ô∏è 0 HUC polygons matched\n",
      "  Nooksack       : ‚ö†Ô∏è 0 HUC polygons matched\n",
      "  Skykomish      : ‚ö†Ô∏è 0 HUC polygons matched\n",
      "  Snoqualmie     : ‚ö†Ô∏è 0 HUC polygons matched\n",
      "  Cedar          : ‚ö†Ô∏è 0 HUC polygons matched\n",
      "  Green          : ‚ö†Ô∏è 0 HUC polygons matched\n",
      "  Puyallup       : ‚ö†Ô∏è 0 HUC polygons matched\n",
      "  Carbon         : ‚ö†Ô∏è 0 HUC polygons matched\n",
      "  Nisqually      : ‚ö†Ô∏è 0 HUC polygons matched\n",
      "  Cowlitz        : ‚ö†Ô∏è 0 HUC polygons matched\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No HUC polygons matched your basin_names. Consider adjusting basin_names or bbox_wa.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasin_name\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m15s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: ‚úì \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(basin_hucs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m HUC polygon(s)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(records) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mNo HUC polygons matched your basin_names. Consider adjusting basin_names or bbox_wa.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     27\u001b[39m subbasins_gdf = gpd.GeoDataFrame(pd.concat(records, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m), crs=\u001b[33m'\u001b[39m\u001b[33mEPSG:4326\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     28\u001b[39m subbasins_gdf = subbasins_gdf.rename(columns={\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mhuc_name\u001b[39m\u001b[33m'\u001b[39m, code_col: \u001b[33m'\u001b[39m\u001b[33mhuc_code\u001b[39m\u001b[33m'\u001b[39m})\n",
      "\u001b[31mRuntimeError\u001b[39m: No HUC polygons matched your basin_names. Consider adjusting basin_names or bbox_wa."
     ]
    }
   ],
   "source": [
    "# Build sub-basin polygons from HUCs and a dissolved basin polygon per basin_name\n",
    "\n",
    "if 'name' not in hucs_all.columns:\n",
    "    raise KeyError(f\"Expected a 'name' column in HUC data. Columns: {list(hucs_all.columns)}\")\n",
    "if code_col not in hucs_all.columns:\n",
    "    raise KeyError(f\"Expected code column '{code_col}' in HUC data. Columns: {list(hucs_all.columns)}\")\n",
    "\n",
    "records = []\n",
    "name_series = hucs_all['name'].astype(str)\n",
    "\n",
    "print('Matching basin names to HUC polygons...')\n",
    "for basin_name in basin_names:\n",
    "    basin_hucs = hucs_all[name_series.str.contains(basin_name, case=False, na=False)].copy()\n",
    "    basin_hucs[code_col] = basin_hucs[code_col].astype(str)\n",
    "\n",
    "    if len(basin_hucs) == 0:\n",
    "        print(f\"  {basin_name:15s}: ‚ö†Ô∏è 0 HUC polygons matched\")\n",
    "        continue\n",
    "\n",
    "    basin_hucs['basin_name'] = basin_name\n",
    "    records.append(basin_hucs[['basin_name', 'name', code_col, 'geometry']])\n",
    "    print(f\"  {basin_name:15s}: ‚úì {len(basin_hucs)} HUC polygon(s)\")\n",
    "\n",
    "if len(records) == 0:\n",
    "    raise RuntimeError('No HUC polygons matched your basin_names. Consider adjusting basin_names or bbox_wa.')\n",
    "\n",
    "subbasins_gdf = gpd.GeoDataFrame(pd.concat(records, ignore_index=True), crs='EPSG:4326')\n",
    "subbasins_gdf = subbasins_gdf.rename(columns={'name': 'huc_name', code_col: 'huc_code'})\n",
    "\n",
    "# Dissolve to one polygon per basin_name (for basin-level station association)\n",
    "basins_gdf = subbasins_gdf.dissolve(by='basin_name', as_index=False)[['basin_name', 'geometry']]\n",
    "\n",
    "# Attach the list of HUC codes used for each basin (string for CSV friendliness)\n",
    "codes_by_basin = (\n",
    "    subbasins_gdf.groupby('basin_name')['huc_code']\n",
    "    .apply(lambda s: ';'.join(sorted(set(s.astype(str)))))\n",
    "    .reset_index()\n",
    "    .rename(columns={'huc_code': 'huc_code'})\n",
    ")\n",
    "basins_gdf = basins_gdf.merge(codes_by_basin, on='basin_name', how='left')\n",
    "\n",
    "total_unique_hucs = subbasins_gdf['huc_code'].nunique()\n",
    "print('-' * 70)\n",
    "print(f\"Matched HUC polygons: {len(subbasins_gdf)} (unique HUC codes: {total_unique_hucs})\")\n",
    "print('  subbasins_gdf rows:', len(subbasins_gdf))\n",
    "print('  basins_gdf rows:', len(basins_gdf))\n",
    "display(basins_gdf[['basin_name', 'huc_code']])\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5a024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e75abbb5",
   "metadata": {},
   "source": [
    "## Step 2: Fetch Seismic Stations (UW, CC)\n",
    "\n",
    "Query station metadata from the FDSN provider and convert results to a GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dec18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch stations for UW + CC networks within bbox_wa\n",
    "client = Client(fdsn_provider)\n",
    "\n",
    "networks = network_filter if network_filter is not None else None\n",
    "network_arg = ','.join(networks) if networks else '*'\n",
    "\n",
    "print(f\"Requesting stations from {fdsn_provider} for network={network_arg}\")\n",
    "inv = client.get_stations(\n",
    "    network=network_arg,\n",
    "    level='station',\n",
    "    minlongitude=bbox_wa[0],\n",
    "    minlatitude=bbox_wa[1],\n",
    "    maxlongitude=bbox_wa[2],\n",
    "    maxlatitude=bbox_wa[3],\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for net in inv:\n",
    "    for sta in net:\n",
    "        rows.append(\n",
    "            {\n",
    "                'network': net.code,\n",
    "                'station': sta.code,\n",
    "                'latitude': float(sta.latitude),\n",
    "                'longitude': float(sta.longitude),\n",
    "                'elevation_m': float(sta.elevation) if sta.elevation is not None else np.nan,\n",
    "                'start_date': getattr(sta, 'start_date', None),\n",
    "                'end_date': getattr(sta, 'end_date', None),\n",
    "            }\n",
    ")\n",
    "\n",
    "stations_df = pd.DataFrame(rows)\n",
    "if len(stations_df) == 0:\n",
    "    raise RuntimeError('No stations returned. Check bbox_wa, network_filter, and fdsn_provider.')\n",
    "\n",
    "# Optional elevation filter\n",
    "if elev_min is not None:\n",
    "    stations_df = stations_df[stations_df['elevation_m'] >= float(elev_min)]\n",
    "if elev_max is not None:\n",
    "    stations_df = stations_df[stations_df['elevation_m'] <= float(elev_max)]\n",
    "\n",
    "stations_gdf = gpd.GeoDataFrame(\n",
    "    stations_df,\n",
    "    geometry=gpd.points_from_xy(stations_df['longitude'], stations_df['latitude']),\n",
    "    crs='EPSG:4326',\n",
    ")\n",
    "\n",
    "print(f\"Stations fetched: {len(stations_gdf)}\")\n",
    "display(stations_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfed49d",
   "metadata": {},
   "source": [
    "## Step 3: Associate Stations with Basins/Sub-basins\n",
    "\n",
    "Spatially join station points to dissolved basins and to sub-basin HUC polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basin-level join (dissolved polygons)\n",
    "stations_basin = gpd.sjoin(\n",
    "    stations_gdf,\n",
    "    basins_gdf[['basin_name', 'geometry']],\n",
    "    how='left',\n",
    "    predicate='within',\n",
    ")\n",
    "stations_basin = stations_basin.drop(columns=[c for c in ['index_right'] if c in stations_basin.columns])\n",
    "\n",
    "# Sub-basin join (HUC polygons)\n",
    "stations_subbasin = gpd.sjoin(\n",
    "    stations_gdf,\n",
    "    subbasins_gdf[['basin_name', 'huc_code', 'huc_name', 'geometry']],\n",
    "    how='left',\n",
    "    predicate='within',\n",
    ")\n",
    "stations_subbasin = stations_subbasin.drop(columns=[c for c in ['index_right'] if c in stations_subbasin.columns])\n",
    "\n",
    "key_cols = ['network', 'station']\n",
    "\n",
    "# If a station falls into multiple polygons, keep the first match deterministically\n",
    "stations_basin_first = stations_basin.sort_values(key_cols).drop_duplicates(subset=key_cols)\n",
    "stations_subbasin_first = stations_subbasin.sort_values(key_cols).drop_duplicates(subset=key_cols)\n",
    "\n",
    "stations_with_basins = stations_gdf.merge(\n",
    "    stations_basin_first[key_cols + ['basin_name']].rename(columns={'basin_name': 'basin_name_basin'}),\n",
    "    on=key_cols,\n",
    "    how='left',\n",
    ")\n",
    "stations_with_basins = stations_with_basins.merge(\n",
    "    stations_subbasin_first[key_cols + ['basin_name', 'huc_code', 'huc_name']].rename(\n",
    "        columns={'basin_name': 'basin_name_subbasin'}\n",
    "    ),\n",
    "    on=key_cols,\n",
    "    how='left',\n",
    ")\n",
    "\n",
    "# Prefer subbasin-derived basin name when present\n",
    "stations_with_basins['basin_name'] = stations_with_basins['basin_name_subbasin'].combine_first(\n",
    "    stations_with_basins['basin_name_basin']\n",
    " )\n",
    "\n",
    "stations_with_basins = stations_with_basins.drop(columns=['basin_name_basin', 'basin_name_subbasin'])\n",
    "\n",
    "assigned = stations_with_basins['basin_name'].notna().sum()\n",
    "print(f\"Stations assigned to a basin: {assigned} / {len(stations_with_basins)}\")\n",
    "display(stations_with_basins[['network','station','basin_name','huc_code','huc_name']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb278f68",
   "metadata": {},
   "source": [
    "## Step 4: QC Map (Basins + Stations)\n",
    "\n",
    "Color-code each basin and plot associated stations on top for visual QA/QC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Create a stable color map for basin names\n",
    "basin_list = list(basins_gdf['basin_name'])\n",
    "cmap = plt.get_cmap('tab20', max(len(basin_list), 1))\n",
    "basin_colors = {name: cmap(i) for i, name in enumerate(basin_list)}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot basins\n",
    "basins_gdf.plot(\n",
    "    ax=ax,\n",
    "    facecolor=basins_gdf['basin_name'].map(basin_colors),\n",
    "    edgecolor='black',\n",
    "    linewidth=0.8,\n",
    "    alpha=0.35,\n",
    ")\n",
    "\n",
    "# Plot stations\n",
    "assigned_mask = stations_with_basins['basin_name'].notna()\n",
    "assigned_gdf = stations_with_basins[assigned_mask].copy()\n",
    "unassigned_gdf = stations_with_basins[~assigned_mask].copy()\n",
    "\n",
    "if len(assigned_gdf) > 0:\n",
    "    assigned_gdf.plot(\n",
    "        ax=ax,\n",
    "        markersize=25,\n",
    "        color=assigned_gdf['basin_name'].map(basin_colors),\n",
    "        edgecolor='white',\n",
    "        linewidth=0.4,\n",
    "        zorder=5,\n",
    "    )\n",
    "\n",
    "if len(unassigned_gdf) > 0:\n",
    "    unassigned_gdf.plot(\n",
    "        ax=ax,\n",
    "        markersize=35,\n",
    "        color='black',\n",
    "        marker='x',\n",
    "        zorder=6,\n",
    "    )\n",
    "\n",
    "# Legend\n",
    "legend_handles = [mpatches.Patch(color=basin_colors[n], label=n) for n in basin_list]\n",
    "if len(unassigned_gdf) > 0:\n",
    "    legend_handles.append(mpatches.Patch(color='black', label='Unassigned (outside basins)'))\n",
    "ax.legend(handles=legend_handles, loc='upper right', frameon=True)\n",
    "\n",
    "ax.set_title('Basins (dissolved) + UW/CC Stations (color-coded by basin)')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# Tighten to bbox (with a small pad)\n",
    "minlon, minlat, maxlon, maxlat = bbox_wa\n",
    "pad_lon = (maxlon - minlon) * 0.02\n",
    "pad_lat = (maxlat - minlat) * 0.02\n",
    "ax.set_xlim(minlon - pad_lon, maxlon + pad_lon)\n",
    "ax.set_ylim(minlat - pad_lat, maxlat + pad_lat)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "qc_path = output_dir / 'qc_basins_stations.png'\n",
    "fig.savefig(qc_path, dpi=200, bbox_inches='tight')\n",
    "print(f\"Saved QC map: {qc_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109e3db",
   "metadata": {},
   "source": [
    "## Step 5: Export CSV\n",
    "\n",
    "Write station metadata + basin/HUC assignment to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1154e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export station assignments to CSV\n",
    "csv_path = output_dir / 'stations_by_basin.csv'\n",
    "\n",
    "export_cols = [\n",
    "    'network',\n",
    "    'station',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'elevation_m',\n",
    "    'basin_name',\n",
    "    'huc_code',\n",
    "    'huc_name',\n",
    "    'start_date',\n",
    "    'end_date',\n",
    "]\n",
    "\n",
    "stations_out = stations_with_basins.copy()\n",
    "for c in export_cols:\n",
    "    if c not in stations_out.columns:\n",
    "        stations_out[c] = np.nan\n",
    "\n",
    "stations_out = pd.DataFrame(stations_out[export_cols])\n",
    "stations_out.to_csv(csv_path, index=False)\n",
    "print(f\"Wrote CSV: {csv_path} ({len(stations_out)} rows)\")\n",
    "\n",
    "display(stations_out.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
